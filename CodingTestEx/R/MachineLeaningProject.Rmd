---
title: "데이터 분석을 통한 머신러닝 알고리즘 구현"
author: "youngchan"
date: '2021 9 7~2021 9 27'
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
#install.packages("languageserver")
install.packages("PerformanceAnalytics")
install.packages("GGally")
install.packages("fmsb")
install.packages("dplyr")
install.packages("reshape2")
install.packages("ggplot2")
install.packages("reshape")
install.packages("ROCR")
install.packages("gridExtra")
install.packages("glmnet")
install.packages("caret")
install.packages("e1071")
install.packages("gmodels")
install.packages("class")
install.packages("rpart.plot")


library(rpart.plot)
library(class)
library(gmodels)
library(caret)
library(e1071)
library(ROCR)
library(reshape)
library(ggplot2)
library(reshape2)
library(dplyr)
library(fmsb)
library(GGally)
library(PerformanceAnalytics)
library(tidyverse)
library(gridExtra)
library(MASS)
library(glmnet)
library(randomForest)
library(gbm)
library(rpart)
library(boot)
library(data.table)
library(ROCR) 

```

# WDBC 데이터셋의 diagnosis 진단 변수는 목표변수(반응변수, 종속변수, Output 변수)로서 
# 2개의 class (악성 Malignant, 양성 Benign)를 가진 범주형 자료로서, 30개의 연속형 설명변수를 
# 사용해서 2개의 class (악성, 양성 여부)를 분류(classification) 하는 해야합니다. 
# 30개의 설명변수를 사용하여 유방암 악성(Malignant)에 속할 0~1 사이의 확률값을 계산하여 진단하는 분류/예측 모델을 만들어보겠습니다.

```{r cars}
data <- read.csv("C:/Users/inp032/Desktop/회사자료/Ai 프로젝트/data.csv")
custom_data <- read.csv("C:/Users/inp032/Desktop/회사자료/Ai 프로젝트/perfectdata.csv")
df <- read.csv("C:/Users/inp032/Desktop/회사자료/Ai 프로젝트/data.csv")

data
head(data)
data.frame(data)
str(data)
summary(data)

attach(data)
```




```{r , echo=FALSE}
# 결측값 찾아내는 함수
colSums(is.na(data))
## 중복 된 데이터가 있으면 시스템 기능에 악영향을 미쳐 도움이되지 않는 시스템에 대한 정보를 반환합니다.
sum(duplicated(data))

## 비율을 알아보는 이유: 데이터가 불균형하게 악성 비율이 1%나 2% 이런식으로 나와버리면, majority undersampling이나 
## SMOTE 알고리즘을 사용하여 언더샘플링 해야하는데 그렇게 되면 데이터 수가 감소하여 성능이 저하 될 수 있다.
# disagnosis 테이블화하여 악성과 양성의 비율을 알아본다
table(diagnosis); cat("total :", margin.table(table(diagnosis)))
# 퍼센트테이블로 전환하여 퍼센테이지로 확인

prop.table(table(diagnosis))
```


## 설명변수간의 다중공선성이 있는지 알아보겠습니다.회귀모형에서는 설명변수간 독립성을 가정합니다. 
## 독립성 가정 만족 여부를 만족하지 않을 경우 조치하기 위함입니다.
```{r , echo=FALSE}

 Y <- ifelse(data$diagnosis == 'M', 1, 0)
 X <- data[,c(3:32)]
 
 #전체의 변수간의 산점도
chart.Correlation(X[,c(1:30)], histogram=TRUE, col="grey10", pch=1)

# 평균간의 산점도
chart.Correlation(X[,c(1:10)], histogram=TRUE, col="grey10", pch=1)

# 표준편차간의 산점도
chart.Correlation(X[,c(11:20)], histogram=TRUE, col="grey10", pch=1)

# 최악의 값의 산점도
chart.Correlation(X[,c(21:30)], histogram=TRUE, col="grey10", pch=1)

## 분석결과
## 상관관계가 0.9 이상으로 보이는 높은 상관관계를 갖고있는 변수들이 다중공선성이 의심 됨.

##상관관계를 파악하기위해 히트맵을 그려봤습니다. -1~1사이의 값이 들어있고 높은 상관관계를 갖고 있는 변수를 지워주기 위함입니다.
ggcorr(X, name="corr", label=T)

```

##분산팽창지수
# 설명변수별 분산팽창지수를 구했을 때 가장 큰 VIF값이 5이상이면 다중
# 공선성이 있다고 보고, 10이상이면 다중공선성이 심각하다고 평가합니다.
# 다중공선성이 높은 다시말해 분산팽창지수가 10이상인 설명변수를 모두 제거하는 방법을 사용하겠습니다.
```{r , echo=FALSE}
VIF(lm(radius_mean ~ .,data=X))
VIF(lm(texture_mean ~ .,data=X))

require(fmsb)
vif_func <- function(in_frame,thresh=10, trace=F,...){
  require(fmsb)
  if(class(in_frame) != 'data.frame') in_frame<-data.frame(in_frame)
  vif_init <- vector('list', length = ncol(in_frame))
  names(vif_init) <- names(in_frame)
  var_names <- names(in_frame)
  
  for(val in var_names){
    regressors <- var_names[-which(var_names == val)]
    form <- paste(regressors, collapse = '+')
    form_in <- formula(paste(val,' ~ .'))
    vif_init[[val]] <- VIF(lm(form_in,data=in_frame,...))
  }
  vif_max<-max(unlist(vif_init))

  if(vif_max < thresh){

    if(trace==T){ #print output of each iteration
      prmatrix(vif_init,collab=c('var','vif'),rowlab=rep('', times = nrow(vif_init) ),quote=F)
      cat('\n')
      cat(paste('All variables have VIF < ', thresh,', max VIF ',round(vif_max,2), sep=''),'\n\n')
    }
    return(names(in_frame))
  }
  else{
    in_dat<-in_frame
    #backwards selection of explanatory variables, stops when all VIF values are below 'thresh'
    while(vif_max >= thresh){

      vif_vals <- vector('list', length = ncol(in_dat))
      names(vif_vals) <- names(in_dat)
      var_names <- names(in_dat)
      
      for(val in var_names){
        regressors <- var_names[-which(var_names == val)]
        form <- paste(regressors, collapse = '+')
        form_in <- formula(paste(val,' ~ .'))
        vif_add <- VIF(lm(form_in,data=in_dat,...))
        vif_vals[[val]] <- vif_add
      }
      max_row <- which.max(vif_vals)

      #max_row <- which( as.vector(vif_vals) == max(as.vector(vif_vals)) )
      vif_max<-vif_vals[max_row]
      
      if(vif_max<thresh) break

      if(trace==T){ #print output of each iteration
        vif_vals <- do.call('rbind', vif_vals)
        vif_vals
        prmatrix(vif_vals,collab='vif',rowlab=row.names(vif_vals),quote=F)
        cat('\n')
        cat('removed: ', names(vif_max),unlist(vif_max),'\n\n')
        flush.console()
      }
      in_dat<-in_dat[,!names(in_dat) %in% names(vif_max)]
    }
    return(names(in_dat))
  }
}

```



```{r , echo=FALSE}

#다중공선성이 높은 컬럼 지워주기
data_custom <- vif_func(X, thresh=10, trace=T) 
#남은 데이터 갯수 확인하기
length(data_custom)

X_2 <- X[, data_custom]


ggcorr(X_2, name="corr", label=T)


```

## 데이터 스케일링을 진행해보도록 하겠습니다. 데이터가 값이 너무 크거나 혹은 작은 경우에 
## 모델 알고리즘 학습과정에서 0으로 수렴하거나 무한으로 발산 해 버릴 우려가 있기 때문에 진행하였습니다.

```{r , echo=FALSE}
# MAX 값을 통해 outlier 값을 확인할 수 있다. 로지스틱 회귀모형으로 적합할 계획이므로
# 따로 안다뤄도 됨.
X_3 <- scale(X_2)
summary(X_3)
```

## T-test를 이용하여 p-value값을 비교하고 효용성이 없는 설명변수를 제거 하도록 하겠습니다.

```{r , echo=FALSE}
X_names <- names(data.frame(X_3))
X_names
t.test_p.value_df <- data.frame()

for (i in 1:length(X_names)) { 
  t.test_p.value <- t.test(data[,X_names[i]] ~ data$diagnosis, var.equal = TRUE)$p.value
  t.test_p.value_df[i,1] <- X_names[i]
  t.test_p.value_df[i,2] <- t.test_p.value}
  
colnames(t.test_p.value_df) <- c("x_var_name", "p.value")
t.test_p.value_df

```


```{r, echo=FALSE}
## 17개의 변수들을 오름차순으로 정렬하였습니다.
## 정렬된 변수들 중에p-value가 0.05 보다 큰 값을 가지는 설명변수인 'symmetry_se', 'texture_se', 'fractal_dimension_mean', 'smoothness_se', 'fractal_dimension_se' 의 5개 설명변수는 1차로 제거하고, 나머지 12개 설명변수만 로지스틱 회귀모형 적합하는데 사용하도록 하겠습니다.
arrange(t.test_p.value_df, p.value)

t.test_filtered <- t.test_p.value_df$p.value < 0.05
X_names_filtered <- X_names[t.test_filtered]
X_4 <- data.frame(X_3[, X_names_filtered])
X_4
length(X_4)
```


##boxflot 
```{r, echo=FALSE}

## dataframe화 시켜서 모형에 들어갈 준비를 함.
t.test_p.value_df.sorted_2 <- arrange(t.test_p.value_df[t.test_filtered,], desc(p.value))
t.test_p.value_df.sorted_2


x_names_sorted <- t.test_p.value_df.sorted_2$x_var_name
x_names_sorted

X_5 <- X_4[x_names_sorted] # rearrange column order for plotting below 

wdbc_2 <- data.frame(Y, X_5)

str(wdbc_2)
#그 다음으로 reshape2 패키지의 melt() 함수를 사용해서 데이터를 세로로 길게 재구조화한 다음에, 
#ggplot2패키지의 ggplot() + geom_boxplot() 함수를 사용하여 박스 그림을 그렸습니다. 

wdbc_2_melt <- melt(wdbc_2, id.var = "Y")


## p-value 값이 낮았던 순으로 위에서 부터 정렬되고, 파란색이 악성, 빨간색이 양성인데 
## p-value 값이 높아지면서 양성과 악성의 관계가 가까워지는 것을 확인할 수 있었습니다.
## 그래프를 한 눈에 봤을 때도 t-test의 결과에 따라 변수들의 양성과 악성을 통한 변수들간의 상관관계를 파악할 수 있었습니다.
 ggplot(data = wdbc_2_melt[wdbc_2_melt$value < 3,], aes(x=variable, y=value)) +
   geom_boxplot(aes(fill=as.factor(Y))) +
   theme_bw() + # white background
   coord_flip() # flip the x & y-axis
```


```{r, echo=FALSE}
wdbc_12 <- data.frame(Y, data[,x_names_sorted])

str(wdbc_12)

```


```{r, echo=FALSE}
ggplot(data=wdbc_2, aes(x=concave.points_mean, y=area_worst, colour=as.factor(Y), alpha=0.5)) +

   geom_point(shape=19, size=3) +

   ggtitle("Scatter Plot of concave.points_mean & area_worst by Y")


```

####              로지스틱 회귀                ####


##모형 적합에 사용할 훈련용 데이터셋(training set), 모형 성능 평가에 사용할 테스트 
##데이터셋(test set) 으로 나누어 보겠습니다.training : test 를 8:2 비율로 나누어서 분석하였습니다.
##만약 training set으로 훈련한 모델의 정확도는 높은데 test set을대상으로한 정확도는 낮다면 과적합을 의심해볼 수 있습니다. 
```{r, echo=FALSE}

set.seed(123) # for reproducibility
train <- sample(1:nrow(wdbc_12), size=0.8*nrow(wdbc_12), replace=F)

test <- (-train)

Y.test <- Y[test]

```



```{r}

binomial(link = "logit")  # logistic regression model

poisson(link = "log") # poisson regression model

gaussian(link = "identity") # linear regression model

Gamma(link = "inverse") # gamma regression model

inverse.gaussian(link = "1/mu^2")

quasi(link = "identity", variance = "constant")

quasibinomial(link = "logit")

quasipoisson(link = "log") 


```



####              로지스틱 회귀                ####

##기존 12개의 데이터로 로지스틱 회귀분석 모형을 만들어본다
```{r, echo=FALSE}
# train with training set

glm.fit <- glm(Y ~ .,  data = wdbc_12,  family = binomial(link = "logit"), subset = train)


summary(glm.fit)


```

##후진제거법으로 영향이 적은 변수들을 지워나간다.
#첫번째 제거
```{r, echo=FALSE}
glm.fit.2 <- glm(Y ~ concave.points_mean + area_worst + 

                    perimeter_se + smoothness_worst + symmetry_worst +

                    texture_mean + smoothness_mean + symmetry_mean + 

                    fractal_dimension_worst + compactness_se + concavity_se, 

                  data = wdbc_12, 

                  family = binomial(link = "logit"), 

                  subset = train)

summary(glm.fit.2)

```

#두번째제거
```{r, echo=FALSE}
glm.fit.3 <- glm(Y ~ concave.points_mean + area_worst + 

                    perimeter_se + smoothness_worst + symmetry_worst +

                    texture_mean + smoothness_mean + 

                   fractal_dimension_worst + compactness_se + concavity_se, 

                 data = wdbc_12, 
                  family = binomial(link = "logit"), 

                 subset = train)


summary(glm.fit.3)

```

#세번째제거
```{r, echo=FALSE}
glm.fit.4 <- glm(Y ~ concave.points_mean + area_worst + 

                    perimeter_se + smoothness_worst + symmetry_worst +

                    texture_mean + smoothness_mean + 

                    compactness_se + concavity_se, 

                  data = wdbc_12, 

                  family = binomial(link = "logit"), 

                  subset = train)
summary(glm.fit.4)
```

#네번째제거
#결과값이 이탈도가 너무 크게나오고, AIC도 높게 나와서 세번째 제거 까지만한 데이터를 사용하겠다.
```{r, echo=FALSE}
glm.fit.5 <- glm(Y ~ concave.points_mean + area_worst + smoothness_worst + 

                    symmetry_worst + texture_mean + smoothness_mean + 

                    compactness_se + concavity_se, 

                  data = wdbc_12, 

                  family = binomial(link = "logit"), 

                  subset = train)

summary(glm.fit.5)
```


```{r, echo=FALSE}

ORtable <- function(x,digits = 2) { +
  suppressMessages(a <- confint(x))
  result=data.frame(exp(coef(x)),exp(a))
  result=round(result,digits)
  result=cbind(result,round(summary(x)$coefficient[ ,4],3))
  colnames(result)=c("OR","2.5","97.5","p")
  result
}

ORtable(glm.fit.4)

Y <- as.integer(ifelse(wdbc_12$Y == 0, 0, 1)) 

##시각화
require(survival)
out1=glm(Y ~ concave.points_mean+area_worst +perimeter_se +smoothness_worst+symmetry_worst+texture_mean+ smoothness_mean+compactness_se,data=wdbc_2)
out2=glm(status~rx+node4,concavity_se ,data=wdbc_2)
ORplot(out1,type=2,show.CI=TRUE,xlab="xlab",main="odds-rate")
ORplot(out2,type=1,main="Main Title")
ORplot(out1,type=2,show.CI=TRUE,main="Main Title")
ORplot(out1,type=3,show.CI=TRUE,main="Main Title",sig.level=0.05)
ORplot(out1,type=3,show.CI=TRUE,main="Main Title",sig.level=0.05,
       pch=1,cex=2,lwd=4,col=c("red","blue"))

```

##결과 값을 보면 양성 61개는 모두 양성으로 전부 정확하게 분류하였으며, 악성 52개에 대해서는 51개를 악성으로 
##분류하였고 1개는 양성으로 오분류 하였습니다.
##따라서 정확도(accuracy) = (TP + TN) / N = (61 + 52)/ 114 = 0.9912281 로서 매우 높게 나왔습니다. 유방암 예측(악성, 양성 분류) 로지스틱 회귀모형이 잘 적합되었네요.
```{r, echo=FALSE}
glm.probs <- predict(glm.fit.4, wdbc_12[test,], type="response")

 glm.probs[1:20]

 glm.pred <- rep(0, nrow(wdbc_12[test,]))

 glm.pred[glm.probs > .5] = 1

# 분류 테이블
table(Y.test, glm.pred)
# 정확도 평균 결과
mean(Y.test == glm.pred)


```

## 마지막으로 ROC 커브를 그려보겠습니다. AUC가 0.9912281로 아주 잘 적합된 모형이 나옵니다
```{r,echo=FALSE}
pr <- prediction(glm.probs, Y.test)

prf <- performance(pr, measure = "tpr", x.measure = "fpr")

plot(prf, main="ROC Curve")

```



####              랜덤 포레스트                ####

#랜덤포레스트나, 의사결정모형은 변수를 임의(랜덤하게)로 선택하여 다른 트리를 만들고 결과를 생성하기 때문에 이 과정에서 가장 
# 중요한 속성이 선택되고 비슷한 경향의 다중공선성문제를 해결한다. 그래서, 다중공선성문제를 굳이 해결할 필요가 없다.
```{r, echo=FALSE}

##데이터 훈련, 검증, 테스트세트 구분하기
set.seed(1810)

custom_data <- data %>% dplyr::select(-id)
custom_data$diagnosis <- factor(ifelse(data$diagnosis == 'B', 0, 1))
n <- nrow(custom_data)
idx <- 1:n
training_idx <- sample(idx, n * .60)
idx <- setdiff(idx, training_idx)
validate_idx <- sample(idx, n * .20)
test_idx <- setdiff(idx, validate_idx)
training <- custom_data[training_idx,]
validation <- custom_data[validate_idx,]
test <- custom_data[test_idx,]
```

##나무모형
#랜덤포레스트는 의사결정나무에 배깅 플러스알파를 적용시킨 모델이다.
#먼저 의사결정나무를 만들고 단계별로 모든 변수가 아닌 랜덤하게 결정된 부분집합의 변수들을 선택한다.
```{r, echo=FALSE}

data_tr <- rpart(diagnosis ~ ., data = training)
data_tr

```


#나무그리기
```{r, echo=FALSE}
train <- sample( 1:300, 100)
tree <- rpart(Y ~ concave.points_mean+area_worst +perimeter_se +smoothness_worst+symmetry_worst+texture_mean+ smoothness_mean+compactness_se, data=wdbc_2, subset = train, method = "class")
rpart.plot(tree)
printcp(tree)

pruned_tree <- prune(tree, cp = 0.1)
predict(pruned_tree, wdbc_2[-train1,], type = "class")
(tt <- table(wdbc_2$Y[-train1], predict(pruned_tree, wdbc_2[-train1,], type = "class")))


opar <- par(mfrow = c( 1,1), xpd = NA)


plot(data_tr)
text(data_tr, use.n = TRUE)

```

#나무모형 평가하기
# 0.9250484로 성능이 좋지 못하다
```{r, echo=FALSE}

yhat_tr <- predict(data_tr, validation)
y_obs <- as.numeric(as.character(validation$diagnosis))
yhat_tr <- yhat_tr[,"1"]
pred_tr <- prediction(yhat_tr, y_obs)
performance(pred_tr, "auc")@y.values[[1]]


```


#랜덤 포레스트
#랜덤포리스트는 배깅과 같다. 유일한 차이점이라면 데이터 샘플을 복원추출할때 모든 설명변수를 다 사용하는 배깅과 달리 a개의 설명변수만 고려하여 분할을 고려한다. 보통 설명변수 갯수 a는 전체 변수 p의 제곱근을 사용한다.(예: 13개 변수라면 4개 정도 사용) 심증이 강한 설명변수라고 특혜를 주지 않고 랜덤하게 변수를 사용된다.
#랜덤포레스트 적용하기
```{r, echo=FALSE}
set.seed(1810)
custom_data

data_rf <- randomForest(diagnosis ~ ., training)

```


#랜덤포레스트 시각화
#초록색 선은 단순 배깅으로 만든 트리이고
#검정색 선은 OOB 즉, 배깅에 사용되지 않은 데이터들로 만든 트리입니다.
#마지막으로 오차가 가장 적은 빨간색 선은 배깅에 사용되지 않은 데이터로 랜덤포레스트를 적용한 결과이다.
```{r,echo=FALSE}
opar <- par(mfrow=c(1,2))

plot(data_rf)
#변수들의 중요도는 높을 수록 의사결정에 큰 영향을 미치,  MeanDecreaseGini는 Random Forest에서 가지를 칠때 얼마나 정확도가 올라가는 지를 나타내는 지표이다.
varImpPlot(data_rf)
par(opar) 

```

# 랜덤포레스트 모형평가
# 정확도 데이터 0.9906512
```{r, echo=FALSE}
print(data_rf)

table(predict(data_rf), training$diagnosis)
breastPred <- predict(data_rf, newdata = training)

table(breastPred, test$diagnosis)

importance(data_rf)
varImpPlot(data_rf)



yhat_rf <- predict(data_rf, newdata=validation, type='prob')[,'1']
pred_rf <- prediction(yhat_rf, y_obs)
performance(pred_rf, "auc")@y.values[[1]]
print(yhat_rf)

pr <- prediction(data_rf, Y.test)

prf <- performance(pr, measure = "tpr", x.measure = "fpr")

plot(prf, main="ROC Curve")
perf_rf <- performance(pred_rf, measure="tpr", x.measure="fpr")
plot(perf_rf, add=TRUE, col='red')
```


####              kNN                ####

# 데이터를 섞어서 랜덤하게 만들어줍니다.
# Id값 제거
```{r, echo=FALSE}
set.seed(123)
wbcd_shuffle <- data[sample(nrow(data)), ]
wbcd2 <- wbcd_shuffle[-1]
```


## 정규화, 데이터프레임화
# 데이터를 정규화 하지않으면, 데이터의 분포가 흩어져있기 때문에 연산속도가 느려지고,  먼저 모든 데이터에 평균값을
# 빼주고 x,y,z,a 좌표에 골고루 분산시켜줍니다. 이렇게 정규화를 시켜주면 데이터의 학습률이 올라갑니다.
```{r, echo=FALSE}
normalize <- function(x){
  return((x-min(x)) / (max(x) - min(x)))
}
ncol <- which(colnames(wbcd2) == "diagnosis")
# factor인 label을 제외하고 normalize한다.
wbcd_n <- as.data.frame(lapply(wbcd2[-ncol], normalize))

# 정규화한 data테이블과 제외한 label을 합쳐준다
wbcd_n <- cbind(wbcd2[1], wbcd_n)
summary(wbcd_n)
```


#train 데이터와 테스트 데이터로 나눈다 (지도학습이기 때문)
```{r , echo=FALSE}
train <- wbcd_n[1:500,c(-1,-2)]
test <- wbcd_n[501:569,c(-1,-2)]
```


#train 데이터와 test데이터의 수를 맞춰서 label을 나눈다.
#k의 값을 바꿔가며 모형을 적용해보면 2일때 가장 좋은 정확도가 도출된다.

```{r, echo=FALSE}

result <- knn(train, test, wbcd_n[1:500, c("diagnosis")], k=15)
result


test_label <- wbcd_n[501:569,c("diagnosis")]
table(test_label, result)


## 해석
##양성이라고 판단하였는데, 악성이 있을 확률(5.6%)
CrossTable(test_label, result)
```

